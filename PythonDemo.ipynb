{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Analyze an image with Computer Vision API using Python\n\n#### With the Analyze Image method, you can extract visual features based on image content. You can upload an image or specify an image URL and choose which features to return, including:\n\n- A detailed list of tags related to the image content.\n- A description of image content in a complete sentence.\n- The coordinates, gender, and age of any faces contained in the image.\n- The ImageType (clip art or a line drawing).\n- The dominant color, the accent color, or whether an image is black & white.\n- The category defined in this taxonomy.\n- Does the image contain adult or sexually suggestive content?\n- Analyze an image\n\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To use the Computer Vision API, you need a subscription key. You can get free subscription keys [here](https://docs.microsoft.com/azure/cognitive-services/Computer-vision/Vision-API-How-to-Topics/HowToSubscribe)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To begin analyzing images, replace subscription_key with a valid API key that you obtained earlier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "subscription_key = \"940ce6e9aed5479bbbf95bab9c6713c6\"\nassert subscription_key",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, ensure that region assigned to the **vision_base_url** variable corresponds to the correct endpoint for your API key (westus, westcentralus, etc.). You can find the correct endpoint in the Azure Portal under the QuickStart panel for the service you created."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vision_base_url = \"https://uksouth.api.cognitive.microsoft.com/vision/v2.0/\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next build the full API call, by appending the command you wish to use to the vision_base_url variable.\n\nThe image analysis URL looks like the following [(see REST API docs here)](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/56f91f2e778daf14a499e1fa).\n\nhttps://[location].api.cognitive.microsoft.com/vision/v2.0/analyze[?visualFeatures][&details][&language]"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vision_analyze_url = vision_base_url + \"analyze\"\nprint(vision_analyze_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To begin analyzing an image, set image_url to the URL of any image that you want to analyze."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://raw.githubusercontent.com/christinaleo-msft/SQLSoton/master/DemoImages/wildwind.jpg\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following block uses the requests library in Python to call out to the Computer Vision analyze API and return the results as a JSON object. The API key is passed in via the headers dictionary and the types of features to recognize via the params dictionary. To see the full list of options that can be used, refer to the REST API documentation for image analysis."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import requests\n\nheaders  = {'Ocp-Apim-Subscription-Key': subscription_key }\nparams   = {'visualFeatures': 'Categories,Description,Color'}\ndata     = {'url': image_url}\nresponse = requests.post(vision_analyze_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()\nanalysis = response.json()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `analysis` object contains various fields that describe the image. The most relevant caption for the image can be obtained from the `descriptions` property."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(analysis)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "image_caption = analysis[\"description\"][\"captions\"][0][\"text\"].capitalize()\nconfidence = analysis[\"description\"][\"captions\"][0][\"confidence\"]\n\nprint(image_caption)\nprint(confidence)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following lines of code display the image and overlay it with the inferred caption."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,15))\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n_ = plt.title(image_caption, size=\"x-large\", y=-0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n\n## -------------------------------------------------------------------------------------------------\n\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n\n\n\n\n\n\n\n## Text recognition with Computer Vision API \nUse the [Recognize Text method](https://westcentralus.dev.cognitive.microsoft.com/docs/services/5adf991815e1060e6355ad44/operations/587f2c6a154055056008f200) to asynchronously detect handwritten or printed text in an image and extract recognized characters into a machine-usable character stream.\n\nSet image_url to point to the image to be recognized."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "image_url = \"https://raw.githubusercontent.com/christinaleo-msft/SQLSoton/master/DemoImages/handwriting.JPG\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The service endpoint for the text recognition service can be constructed as follows:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "text_recognition_url = vision_base_url + \"recognizeText\"\nprint(text_recognition_url)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The handwritten text recognition service can be used to recognize the text in the image. In the `params` dictionary, you can set `mode` to `Printed` to recognize only printed text."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "headers  = {'Ocp-Apim-Subscription-Key': subscription_key}\nparams   = {'mode' : 'Handwritten'}\ndata     = {'url': image_url}\n\nresponse = requests.post(text_recognition_url, headers=headers, params=params, json=data)\nresponse.raise_for_status()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The text recognition service does not return the recognized text by itself. Instead, it returns immediately with an \"Operation Location\" URL in the response header that must be polled to get the result of the operation."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "operation_url = response.headers[\"Operation-Location\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After obtaining the `operation_url`, you can query it for the analyzed text. The following lines of code implement a polling loop in order to wait for the operation to complete. Notice that the polling is done via an HTTP `GET` method instead of `POST`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import time\n\nanalysis = {}\nwhile not \"recognitionResult\" in analysis:\n    response_final = requests.get(response.headers[\"Operation-Location\"], headers=headers)\n    analysis       = response_final.json()\n    time.sleep(1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(analysis)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, the recognized text along with the bounding boxes can be extracted as shown in the following line of code. An important point to note is that the handwritten text recognition API returns bounding boxes as **polygons** instead of **rectangles**. Each polygon is p is defined by its vertices specified using the following convention:\n\np = [x1, y1, x2, y2, ..., xN, yN]"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "polygons = [(line[\"boundingBox\"], line[\"text\"]) for line in analysis[\"recognitionResult\"][\"lines\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, the recognized text can be overlaid on top of the original image using the extracted polygon information. Notice that `matplotlib` requires the vertices to be specified as a list of tuples of the form:\n\np = [(x1, y1), (x2, y2), ..., (xN, yN)]\n\nand the post-processing code transforms the polygon data returned by the service into the form required by `matplotlib`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,15))\nimage = Image.open(BytesIO(requests.get(image_url).content))\nplt.imshow(image)\nplt.axis(\"off\")\n_ = plt.title(\"Original Image\", size=\"x-large\", y=-0.1)\n\n\nfrom matplotlib.patches import Polygon\n\nplt.figure(figsize=(15,15))\n\nimage  = Image.open(BytesIO(requests.get(image_url).content))\nax     = plt.imshow(image)\nfor polygon in polygons:\n    vertices = [(polygon[0][i], polygon[0][i+1]) for i in range(0,len(polygon[0]),2)]\n    text     = polygon[1]\n    patch    = Polygon(vertices, closed=True,fill=True, linewidth=2, color='y')\n    ax.axes.add_patch(patch)\n    plt.text(vertices[0][0], vertices[0][1], text, fontsize=20, va=\"top\")\n_ = plt.axis(\"off\")\n_ = plt.title(\"Extracted Text\", size=\"x-large\", y=-0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "# Your Turn\n\n### All the available Quick Starts\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/\n\n### Some of the simpler ones\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/translator/quickstart-python-translate\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/python\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/bing-image-search/quickstarts/python\n\n### This one needs no coding skills\n\nhttps://www.qnamaker.ai/"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}